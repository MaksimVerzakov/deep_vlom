МЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫЙОД
ЙОДОПИСАНИЕ АЛГОРИТМА CART
УДК 004.6
Описание алгоритма CART
И. М. Андреев
.В статье описывается один из самых известных алгоритмов построения деревьев
решений CART. Алгоритм применяется в
активно развивающемся направлении Data
Mining — добычи знаний из баз данных.
Алгоритм предназначен для решения задач
классификации объектов и построения регрессионной модели. По сравнению с нейросетями, обычно применяемыми для решения подобных задач, алгоритм отличает высочайшая скорость и возможность наглядной
визуализации найденного решения при сравнимой точности классификации. Статья будет полезна специалистам, интересующимся
направлением Data Mining, и разработчикам,
применяющим алгоритмы Data Mining в своих
продуктах.
CART (сокращение от Classification And Regression
Tree) переводится как «Дерево Классификации и Регрессии» — алгоритм бинарного дерева решений, впервые
опубликованный Бриманом и др. в 1984 году [1]. Алгоритм предназначен для решения задач классификации
и регрессии. Существует также несколько модифицированных версий — алгоритмы IndCART и DB-CART. Алгоритм IndCART является частью пакета Ind и отличается от CART использованием иного способа обработки пропущенных значений, он не реализует регрессионную часть алгоритма CART и имеет иные параметры
отсечения. Алгоритм DB-CART базируется на следующей идее: вместо того, чтобы использовать обучающий
набор данных для определения разбиений, оцениваются
распределения входных и выходных значений, и затем
на основе полученных оценок определяются разбиения.
Утверждается, что эта идея дает значительное уменьшение ошибки классификации по сравнению со стандартными методами построения дерева. Основные отличия алгоритма CART от алгоритмов семейства ID3
(http://glossary.basegroup.ru/a/id3.htm) заключены
в следующих представлениях, функциях и механизмах:
бинарном представленим дерева решений;
функции оценки качества разбиения;
механизме отсечения дерева;
алгоритме обработки пропущенных значений;
построении деревьев регрессии.
Бинарное представление дерева
решений
В алгоритме CART каждый узел дерева решений имеет двух потомков. На каждом шаге построения дерева
правило, формируемое в узле, делит заданное множе48
ство примеров (обучающую выборку) на две части —
часть, в которой выполняется правило (потомок — right),
и часть, в которой правило не выполняется (потомок —
left). Для выбора оптимального правила используется
функция оценки качества разбиения.
Предлагаемое алгоритмическое решение
Каждый узел (структура или класс) должен иметь
ссылки на двух потомков: Left и Right (структуры аналогичного вида). Узел также должен содержать идентификатор правила (подробнее о правилах см. ниже), какимлибо образом описывать правую часть правила, содержать информацию о количестве или отношении примеров каждого класса обучающей выборки, «прошедшей»
через узел, и иметь признак терминального узла — листа. Таковы минимальные требования к структуре (классу) узла дерева.
Функция оценки качества разбиения
Построение дерева решений относится к классу обучения с учителем, то есть обучающая и тестовая выборки содержат классифицированный набор примеров. Оценочная функция, используемая алгоритмом CART, базируется на интуитивной идее уменьшения «нечистоты»
(неопределенности) в узле. Имеется в виду такое разбиение, при котором в узле будет как можно больше примеров одного класса и как можно меньше всех других. Это
очень похоже на термин «неопределенность» — энтропия. Но дело в том, что тут используется другая (не энтропийная) мера неопределенности. Потому и пришлось
использовать термин «нечистота».
АВТОР
Андреев Иван
Михайлович
Инженер-программист;
RBaseGroup Labs, г. Рязань
Рассмотрим задачу с двумя классами и узлом, имеющим по 50 примеров одного класса. Пусть узел имеет
максимальную «нечистоту». Если будет найдено разбиение, которое разбивает данные, например, на две подгруппы (40:5 примеров в одной и 10:45 в другой), то интуитивно «нечистота» уменьшится. Она полностью исчезнет, когда будет найдено разбиение, которое создаст
подгруппы 50:0 и 0:50. В алгоритме CART идея «нечистоты» формализована в индексе Gini. Если набор данных T содержит данные n классов, тогда индекс Gini
#3—4 (7—8) 2004
ЙОДМЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫ
И. М. АНДРЕЕВЙОД
Предлагаемое алгоритмическое решение
определяется как
n
p2 ,
i
Gini(T ) = 1 −
i=1
где параметр pi — вероятность класса i в T .
Если набор T разбивается на две части T1 и T2 с числом примеров в каждом N1 и N2 соответственно, то показатель качества разбиения будет равен
Ginisplit (T ) =
N1
N2
Gini(T1 ) +
Gini(T2 ).
N
N
РЕЗЮМЕ
Наилучшим считается то разбиение, для которого
Ginisplit (T ) минимально.
Обозначим через N число примеров в узле-предке, через L и R — число примеров в левом и правом потомке
соответственно, li и ri — число экземпляров i-го класса в левом/правом потомке. Тогда качество разбиения
оценивается по следующей формуле:
n
L R
N 1−
   N
2
li
L
1−
+ 
Ginisplit =
i=1
n
ri
R
i=1
+
2
→ min.
Чтобы уменьшить объем вычислений, формулу для
Ginisplit можно переписать иначе:
Ginisplit =
n
1
1
L 1− 2
N
L
2
li
+R 1−
i=1
1
R2
n
2
ri
.
i=1
Так как умножение на константу не играет роли при минимизации, то
Ginisplit = L −
Ginisplit = N −
1
 ̃
Gsplit =
L
Договоримся, что источник данных, необходимых для
работы алгоритма, представим в виде плоской таблицы,
каждая строка которой описывает один пример обучающей/тестовой выборки. Каждый шаг построения дерева
фактически состоит из совокупности трех трудоемких
операций.
1
L
n
2
li + R −
i=1
1
L
n
i=1
n
2
li +
i=1
2
li +
1
R
1
R
n
1
R
n
2
ri → min,
i=1
n
2
ri
→ min,
i=1
2
ri → max.
i=1
В итоге, лучшим будет то разбиение, для которо ̃
го величина Gsplit максимальна. Реже в алгоритме CART используются другие критерии разбиения:
Twoing, Symmetric Gini и др. (подробнее см. [1]).
Правила разбиения
Вектор предикторных переменных, подаваемый на
вход дерева, может содержать как числовые (порядковые), так и категориальные переменные. В любом случае, в каждом узле разбиение идет только по одной переменной. Если переменная имеет числовой тип, то в узле формируется правило вида xi ≤ c, где c — некоторый порог, который чаще всего выбирается как среднее
арифметическое двух соседних упорядоченных значений
переменной обучающей выборки. Если переменная имеет категориальный тип, то в узле формируется правило xi ∈ V (xi ), где V (xi ) — некоторое непустое подмножество множества значений переменной xi в обучающей
выборке. Следовательно, для n значений числового атрибута алгоритм сравнивает n − 1 разбиений, а для категориального — (2n−1 − 1). На каждом шаге построения дерева алгоритм последовательно сравнивает все
возможные разбиения для всех атрибутов и выбирает
наилучший атрибут и наилучшее разбиение для него.
МАТЕМАТИКА В ПРИЛОЖЕНИЯХ
Задача
Data Mining.
Результаты
Подробно рассмотрен алгоритм построения деревьев
решений CART.
1. Сортировка источника данных по столбцу. Она
необходима для вычисления порога, когда рассматриваемый в текущий момент времени атрибут имеет числовой
тип. На каждом шаге построения дерева число сортировок будет как минимум равно количеству атрибутов
числового типа.
2. Разделение источника данных. После того как найдено наилучшее разбиение, необходимо разделить источник данных в соответствии с правилом формируемого
узла и рекурсивно вызвать процедуру построения для
двух половинок источника данных.
Обе эти операции связаны (если действовать напрямую) с перемещением данных, занимающих значительные объемы памяти. Здесь намеренно источник данных
не называется таблицей, т. к. можно существенно снизить временные затраты на построение дерева, если использовать индексированный источник данных. Обращение к данным в таком источнике происходит не напрямую, а посредством логических индексов строк данных.
Сортировать и разделять такой источник можно с минимальной потерей производительности.
3. Третья операция, занимающая 60–80% времени вы ̃
полнения программы, — вычисление индексов Gsplit для
всех возможных разбиений. Если у вас n числовых атрибутов и m примеров в выборке, то получается таблица
индексов размера n × (m − 1), которая занимает большой
объем памяти. Этого можно избежать, если использовать один столбец для текущего атрибута и одну строку
для лучших (максимальных) индексов для всех атрибутов. Можно и вовсе использовать только несколько числовых значений, получив быстрый, но плохо читаемый
код. Значительно увеличить производительность можно,
если принять во внимание, что L = N −R, li = ni −ri , а li
и ri изменяются всегда и только на единицу при переходе
на следующую строку для текущего атрибута. Другими
словами, подсчет числа классов (а это основная операция) будет выполняться быстро, если знать число экземпляров каждого класса в таблице и при переходе на новую строку таблицы изменять на единицу только число
экземпляров одного класса — класса текущего примера.
Все возможные разбиения для категориальных атрибутов удобно представлять по аналогии с двоичным
представлением числа. Если атрибут имеет n уникальных значений, имеем 2n разбиений. Первое (где все нули) и последнее (все единицы) нас не интересуют, таким образом, получаем 2n − 2 разбиений, а т. к. порядок множеств здесь тоже неважен, получаем, что число первых (с единицы) двоичных представлений равно
(2n − 2)/2 = 2n−1 − 1. Если {A, B, C, D} — все возмож49
МЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫЙОД
ные значения некоторого атрибута X, то для текущего
разбиения, которое имеет представление, например, для
{0, 0, 1, 0, 1}, получаем правило X in {C, E} для правой
ветви и not {0, 0, 1, 0, 1} = {1, 1, 0, 1, 0} = X in {A, B, D}
для левой ветви.
Часто значения атрибута категориального типа представлены в базе как строковые значения. В таком случае
быстрее и удобнее создать кэш всех значений атрибута
и работать не со значениями, а с индексами в кэше.
Механизм отсечения дерева
Наиболее серьезное отличие алгоритма CART от других алгоритмов построения дерева заключено в механизме отсечения дерева (оригинальное название minimal
cost-complexity tree pruning). CART рассматривает отсечение как получение компромисса между получением дерева оптимального размера и получением точной оценки
вероятности ошибочной классификации.
Основная проблема отсечения — большое количество
возможных отсеченных поддеревьев для одного дерева.
Конкретнее, если бинарное дерево имеет |T | листов, то
существует ≈ 1.5028369|T | отсеченных поддеревьев. Таким образом, если дерево имеет хотя бы 1000 листов, то
число отсеченных поддеревьев становится катастрофически большим.
Базовая идея метода — не рассматривать все возможные поддеревья, а ограничиться только «лучшими представителями» согласно приведенной ниже оценке.
Обозначим через |T | число листов (терминальных узлов) дерева, а через R(T ) — ошибку классификации дерева, равную отношению числа неправильно классифицированных примеров к числу примеров в обучающей
выборке. Определим Cα (T ) — полную стоимость (оценку/показатель затраты—сложность) дерева T как
Cα (T ) = R(T ) + α|T |,
где α — некоторый параметр, изменяющийся от 0 до +∞.
Полная стоимость дерева включает две компоненты —
ошибку классификации дерева и штраф за его сложность. Если ошибка классификации дерева неизменна,
то с увеличением α полная стоимость дерева будет увеличиваться. Тогда, в зависимости от α, менее ветвистое
дерево, дающее большую ошибку классификации, может
стоить меньше, чем дерево более ветвистое, но дающее
меньшую ошибку.
Определим Tmax — максимальное по размеру дерево,
которое предстоит обрезать. Если мы зафиксируем значение α, то будем иметь наименьшее минимизируемое
поддерево T (α), для которого выполняются следующие
условия:
1) Cα (T (α)) = minT ≤Tmax Cα (T );
2) if Cα (T ) = Cα (T (α)) then T (α) ≤ T.
Первое условие говорит, что не существует такого поддерева дерева Tmax , которое имело бы меньшую стоимость,
чем T (α) при заданном значении α. Из второго условия следует, что если существует более одного поддерева, имеющего данную полную стоимость, то мы выбираем наименьшее дерево. Можно показать, что для любого
значения α существует такое наименьшее минимизируемое поддерево. Однако эта задача нетривиальна. Из ее
решения следует, что ситуация, когда два дерева достигают минимума полной стоимости и являются несравнимыми (т. е. ни одно из них не является поддеревом
другого), невозможна.
Несмотря на то что α имеет бесконечное число зна50
ЙОДОПИСАНИЕ АЛГОРИТМА CART
чений, существует конечное число поддеревьев дерева
Tmax . Следовательно, можно построить последовательность уменьшающихся поддеревьев дерева Tmax :
T1 > T2 > T3 > ... > {t1 }
(где t1 — корневой узел дерева) такую, что Tk — наименьшее минимизируемое поддерево для α ∈ [αk , αk+1 ). Этот
важный результат показывает, что мы можем получить
следующее дерево в последовательности, применив отсечение к текущему дереву. Последнее позволяет разработать эффективный алгоритм поиска наименьшего минимизируемого поддерева при различных значениях α.
Первое дерево в этой последовательности — наименьшее
поддерево дерева Tmax , имеющее такую же ошибку классификации, как и Tmax , т. е. T1 = T (α = 0). Другими
словами, если разбиение идет до тех пор, пока в каждом узле не останется только один класс, то T1 = Tmax .
Но так как часто применяются методы ранней остановки (prepruning), то может существовать поддерево дерева
Tmax , имеющее такую же ошибку классификации.
Предлагаемое алгоритмическое решение
Алгоритм вычисления T1 из Tmax прост: надо найти
любую пару листов с общим предком, которые могут
быть объединены, т. е. отсечены в родительский узел без
увеличения ошибки классификации,
R(t) = R(l) + R(r),
где r и l — листы узла t. Эту процедуру следует продолжать до тех пор, пока таких пар больше не останется. В
итоге мы получим дерево, имеющее такую же стоимость,
как и Tmax при α = 0, но менее ветвистое, чем Tmax .
Здесь встает вопрос о том, как получить следующее
дерево в последовательности и соответствующее значение α. Обозначим через Tt ветвь дерева T с корневым
узлом t. При каких же значениях α дерево T − Tt будет лучше, чем T ? Если мы сделаем отсечение в узле t,
то вклад этого узла в полную стоимость дерева T − Tt
станет равным Cα ({t}) = R(t) + α. Здесь R(t) = r(t)p(t),
r(t) — ошибка классификации узла t и p(t) — пропорция случаев, которые «прошли» через узел t. Альтернативный вариант: R(t) = m/n, где m — число примеров,
классифицированных некорректно, а n — общее число
классифицируемых примеров для всего дерева.
Вклад Tt в полную стоимость дерева T составит
Cα (Tt ) = R(Tt ) + α|Tt |,
где
R(Tt ) =
R(t ).
t ∈Tt
Дерево T − Tt будет лучше, чем T , в том случае, если
Cα ({t}) = Cα (Tt ), поскольку при этой величине α они
имеют одинаковую стоимость, но T − Tt меньше. Когда
Cα ({t}) = Cα (Tt ), имеем
R(Tt ) + α|Tt | = R(t) + α.
Решая уравнение относительно α, получаем
α=
R(t) − R(Tt )
.
|Tt | − 1
Для любого узла t в T1 при увеличении α, когда
α=
R(t) − R(T1,t )
,
|T1,t | − 1
дерево, полученное отсечением в узле t, будет лучше T1 .
#3—4 (7—8) 2004
ЙОДМЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫ
И. М. АНДРЕЕВЙОД
Основная идея здесь состоит в следующем: вычислим
это значение α для каждого узла в дереве T1 и затем
выберем «слабые связи» (их может быть больше, чем
одна), т. е. узлы, для которых величина
g(t) =
R(t) − R(T1,t )
|T1,t | − 1
È
является одновременно и деревом T1 , все листы не имеют неправильно классифицированные примеры, поэтому
R(T1,t1 ) = 5 0/200 = 0. Здесь |T1,t1 | — количество ли1
стов поддерева с корнем в узле t1 , число которых равно
пяти. В итоге получаем:
g1 (t1 ) = g1 (t2 ) = 
T1 = T (α = 0)
α1 = 0
k=1
while Tk > {root node} do
begin
для всех нетерминальных узлов (!листов) в t ∈ Tk
R(t) − R(T1,t )
g(t) =
|T1,t − 1|
αk+1 = mint gk (t)
Обойти сверху вниз все узлы и обрезать те,
где gk (t) = αk+1 , чтобы получить Tk+1
k =k+1
end
Узлы необходимо обходить сверху вниз, чтобы не отсекать узлы, которые отсекутся сами собой в результате
отсечения n-го предка.
1
,
20
g1 (t5 ) = 
Алгоритм вычисления последовательности деревьев
3
,
20
g1 (t3 ) = 
является наименьшей. Затем отсечем T1 в этих узлах,
чтобы получить T2 — следующее дерево в последовательности. Продолжим этот процесс для полученного дерева
до тех пор, пока не получим корневой узел (дерево, в
котором только один узел).
1/2 − 0
 1
= ,
5−1
 8
1
.
20
Узлы t3 и t5 хранят минимальное значение g1 . Чтобы
получить новое дерево T2 , нужно обрезать T1 в обоих
этих узлах. Полученное дерево изображено на рис. 2.
Продолжим вычислять значение g для T2 :
g2 (t1 ) =
100/200 − (0/200 + 10/200 + 10/200)
2
=
,
3−1
10
60/200 − (0/200 + 10/200)
1
= .
2−1
4
Поскольку минимум хранится в узле t1 , то, обрезая в
t1 , получим корень дерева (T3 = {t1 }). На этом процесс
отсечения заканчивается.
Последовательность значений α имеет следующий
вид: α1 = 0, α2 = 1/20, α3 = 2/10. В итоге T1 является лучшим деревом для α ∈ [0, 1/20), T2 — для
α ∈ [1/20, 2/10) и T3 = {t1 } — для α ∈ [2/10, +∞).
g2 (t2 ) =
Выбор финального дерева
Рис. 1. Дерево Tmax .
В качестве примера вычислим последовательность
поддеревьев и соответствующих значений α для дерева,
изображенного на рис. 1. Очевидно, что T1 = Tmax , т. к.
все листы содержат примеры одного класса, и отсечение
любого узла (t3 или t5 ) приведет к возрастанию ошибки
классификации. Затем вычислим g1 (t) для всех узлов t
в T:
R(t1 ) − R(T1,t1 )
g1 (t1 ) =
,
|T1,t1 | − 1
где R(T1 ) — ошибка классификации. Если t1 превратить
в лист, то следует сопоставить ему какой-либо класс.
Так как число примеров обоих классов одинаково (равно 100), то класс выбираем наугад, в любом случае он
неправильно классифицирует 100 примеров. Всего дерево обучалось на 200-х примерах (100 + 100 = 200
для корневого узла), причем R(T1 ) = m/n, m = 100,
n = 200, R(T1 ) = 1/2. В последней формуле R(T1,t1 ) —
сумма ошибок всех листов поддерева, которая рассчитывается как сумма отношений количества неправильно классифицированных примеров в листе к общему
числу примеров для дерева по всем листам. В примере все делим на 200. Так как поддерево с корнем в t1
МАТЕМАТИКА В ПРИЛОЖЕНИЯХ
Выбор финального дерева заключается в выборе лучшего дерева из последовательности деревьев. В дальнейшем мы будем использовать именно такое дерево. Отметим, что при отсечении дерева не использовались никакие другие данные, кроме тех, на которых строилось первоначальное дерево. Требовались даже не сами данные,
а количество примеров каждого класса, которое «прошло» через узел.
Наиболее очевидным и, возможно, наиболее эффективным является выбор финального дерева посредством
тестирования на тестовой выборке. Естественно, качество тестирования во многом зависит от объема тестовой
выборки и «равномерности» данных, которые попали в
обучающую и тестовую выборки.
Часто можно наблюдать, что последовательность деревьев дает сопоставимые ошибки (такой случай изображен на рис. 3). Представленная длинная плоская последовательность очень чувствительна к данным, которые будут выбраны в качестве тестовой выборки. Чтобы уменьшить эту нестабильность, CART использует
Рис. 2. Дерево T2 .
51
МЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫЙОД
ЙОДОПИСАНИЕ АЛГОРИТМА CART
Альтернативный путь заключается в том, чтобы на
последнем шаге опять использовать (1 − SE)-правило
для выбора финального дерева из последовательности.
Алгоритм обработки пропущенных
значений
Рис. 3. Зависимость ошибки от размера дерева.
(1 − SE)-правило: выберается минимальное по размеру
дерево с Rts в пределах интервала [min Rts , min Rts +SE].
Здесь Rts — ошибка классификации дерева; SE — стандартная ошибка, являющаяся оценкой реальной ошибки:
SE(Rts ) =
Ö
Rts (1 − Rts )
,
ntest
где ntest — число примеров в тестовой выборке. Ситуация
проиллюстрирована на рис. 3.
Перекрестная проверка
Перекрестная проверка (V-fold cross-validation) — оригинальная и наиболее сложная часть алгоритма CART.
Этот способ выбора финального дерева используется, когда набор данных для обучения мал или каждая запись
в нем по-своему «уникальна», так что мы не можем выделить выборку для обучения и выборку для тестирования.
Имея такой случай, следует построить дерево на всех
данных и вычислить α1 , α2 ,... αk и T1 > T2 > ... > TN .
Обозначим через Tk наименьшее минимизируемое поддерево для α ∈ [αk , αk+1 ).
Теперь возникает задача выбора дерева из последовательности при условии, что все имеющиеся данные уже
использованы. Выход тут заключается в том, что вычисление ошибки дерева Tk из последовательности выполняется косвенным путем.
√
√
Шаг 1. Установим β1 = 0, β2 = α2 α3 , β3 = α3 α4 , ...
√
βN −1 = αN −1 αN , βN = ∞. Считается, что βk будет типичным значением для [αk , αk+1 ) и, следовательно, как
значение соответствует Tk .
Шаг 2. Разделим весь набор данных на V групп G1 ,
G2 , ... GV одинакового размера (Бриман [4] рекомендует
брать V = 10). Затем для каждой группы Gi следует
выполнить следующее.
1. Вычислить последовательность деревьев с помощью описанного выше механизма отсечения на всех данных, исключая Gi , и определить T (i) (β1 ), T (i) (β2 ), ...
T (i) (βN ) для этой последовательности.
2. Вычислить ошибку дерева T (i) (βk ) на Gi . Здесь
(i)
T (βk ) означает наименьшее минимизированное поддерево из последовательности, построенное на всех данных, исключая Gi для α = βk .
Шаг 3. Для каждого βk суммировать ошибку T (i) (βk )
по всем Gi (i = 1, V ). Пусть βh будет с наименьшей общей
ошибкой. Так как βh соответствует дереву Th , выбираем Th из последовательности, построенной на всех данных в качестве финального дерева. Показатель ошибки,
вычисленный с помощью перекрестной проверки, можно
использовать как оценку ошибки дерева.
52
Большинство алгоритмов Data Mining предполагает
отсутствие пропущенных значений. На практике это
предположение часто является неверным. Вот только
некоторые из причин, которые могут привести к пропущенным данным:
1) респондент не желает отвечать на некоторые из поставленных вопросов;
2) допущены ошибки при вводе данных;
3) выполнено объединение не совсем эквивалентных
наборов данных.
Часто в таких случаях исключают данные, которые содержат один или несколько пустых атрибутов. Однако
такой подход имеет ряд недостатков.
1. Смещение данных. Если выброшенные данные лежат несколько «в стороне» от оставленных, анализ
может дать смещенные результаты.
2. Потеря мощности. Если количество исключенных
данных большое, точность прогноза сильно уменьшается.
Если дерево используется на неполных данных, необходимо решить следующие вопросы:
1) как определить качество разбиения?
2) в какую ветвь необходимо послать наблюдение, если пропущена переменная, на которую приходится
наилучшее разбиение (построение дерева и тренировка)?
Следует подчеркнуть, что наблюдение с пропущенной
меткой класса бесполезно для построения дерева и будет
выброшено.
Чтобы определить качество разбиения, CART просто игнорирует пропущенные значения. Это «решает»
первую проблему. Но еще следует решить, по какому пути посылать наблюдение с пропущенной переменной, содержащей наилучшее разбиение. С этой целью
CART вычисляет так называемое «суррогатное» разбиение. Оно создает наиболее близкие к лучшему подмножества примеров в текущем узле. Чтобы определить значение альтернативного разбиения как суррогатного, мы
создаем кросс-таблицу:
π(l∗ , l )
π(r∗ , l )
π(l∗ , r )
π(r∗ , r )
В этой таблице величина π(l∗ , l ) обозначает пропорцию
случаев, которые будут посланы в левую ветвь при лучшем s∗ и альтернативном s разбиении, и аналогично для
π(r∗ , r ). Так, π(l∗ , l ) + π(r∗ , r ) обозначает пропорцию
случаев, которые посланы в одну и ту же ветвь для обоих разбиений. Это мера сходства разбиений, характеризующая, насколько хорошо предсказывается путь, по которому послан случай с наилучшим разбиением по сравнению с альтернативным. Если π(l∗ , l ) + π(r∗ , r ) < 0.5,
можно получить лучший суррогат, поменяв левую и правую ветви для альтернативного разбиения. Кроме того,
необходимо заметить, что пропорции в таблице вычислены для случая, когда обе переменные (суррогатная и
альтернативная) наблюдаемы.
Альтернативные разбиения, для которых выполняеся π(l∗ , l ) + π(r∗ , r ) > max(π(r∗ ), π(l∗ )), отсортированы
в нисходящем порядке в плане сходства. Теперь, если
пропущена переменная лучшего разбиения, используем
первую из суррогатных в списке, если пропущена и она,
#3—4 (7—8) 2004
ЙОДМЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫ
И. М. АНДРЕЕВЙОД
то следующую, и т. д. Если пропущены все суррогатные
переменные, используем max(π(r∗ ), π(l∗ )).
На рис. 4 изображен пример лучшего и альтернативного разбиения. Каково значение альтернативного разбиения по супружескому статусу как суррогатного? Примеры 6 и 10 послали оба разбиения влево. Следовательно, π(l∗ , l ) = 2/7. Оба разбиения послали примеры 1 и
4 направо — π(r∗ , r ) = 2/7. Его значение как суррогатного есть π(l∗ , l ) + π(r∗ , r ) = 2/7 + 2/7 = 4/7. Так как
max(π(r∗ ), π(l∗ )) = 4/7, разбиение по супружескому статусу не является хорошим суррогатным разбиением.
Регрессия
Построение дерева регрессии во многом схоже с построением дерева классификации. Сначала строится дерево максимального размера, которое затем обрезается
до оптимального размера.
Основное достоинство деревьев по сравнению с другими методами регрессии — возможность работать с многомерными задачами (см. рис. 5) и задачами, в которых
присутствует зависимость выходной переменной от переменной или переменных категориального типа.
Основная идея — разбиение всего пространства на
прямоугольники (необязательного одинакового размера), в которых выходная переменная считается постоянной. При этом существует сильная зависимость между
объемом обучающей выборки и ошибкой ответа дерева.
Процесс построения дерева происходит последовательно. На первом шаге получаем регрессионную оценку как константу по всему пространству примеров. Константу вычисляем как среднее арифметическое выходной переменной в обучающей выборке. Если обозначить
все значения выходной переменной как Y1 , Y2 , ... Yn , то
регрессионная оценка получается в виде
ˆ
f (x) =
1
n
n
Yi
IR (x),
i=1
где R — пространство обучающих примеров; n — число примеров; IR (x) — индикаторная функция пространства (фактически, набор правил, описывающих попадание переменной x в пространство); пространство R рассматривается как прямоугольник. На втором шаге пространство делится на две части. Выбирается некоторая
переменная xi и, если она принадлежит числовому типу,
определяются
R1 = {x ∈ R : x ≤ a},
R2 = {x ∈ R : x > a}.
Если xi принадлежит категориальному типу и имеет возможные значения A1 , A2 , .. Aq , то выбирается некоторое
подмножество I ⊂ {A1 , ...An } и определяется
R1 = {x ∈ R : x ∈ I},
R2 = {x ∈ R : x ∈ {A1 , A2 , ...Aq }\ I}.
Рис. 4. Лучшее и альтернативное разбиения.
МАТЕМАТИКА В ПРИЛОЖЕНИЯХ
Рис. 5. Пример двумерной задачи.
Регрессионная оценка принимает вид:
ˆ
f (x) =
1
|I1 |
Yi
IR1 (x) +
I1
1
|I2 |
Yi
IR2 (x),
I2
где I1 = {i, xi ∈ R1 } и |I1 | — число элементов в I1 .
Оценкой лучшего разбиения служит сумма квадратов
разностей
n
E=
2
ˆ
Yi − f (xi ) ,
i=1
которую нужно минимизировать. Процесс разбиения
продолжается до тех пор, пока в каждом подпространстве не останется малое число примеров или пока сумма
квадратов разностей не станет меньше некоторого заданного порога.
Отсечение и выбор финального дерева производятся
аналогично тем же процедурам для дерева классификации. Единственное отличие — определение ошибки ответа дерева
n
2
1
ˆ
ˆ
Yi − f (xi ) ,
R(f ) =
n i=1
или, иначе говоря, среднеквадратичной ошибки ответа.
Стоимость дерева равна
ˆ
ˆ
ˆ
Cα (f ) = R(f ) + α|f |.
Остальные операции происходят аналогично построению
дерева классификации.
Заключение
Алгоритм CART успешно сочетает в себе качество построенных моделей и, при удачной реализации, высокую
скорость их построения. Также он включает в себя уникальные методики обработки пропущенных значений и
построения оптимального дерева на основе совокупности алгоритмов cost-complexity pruning и V-fold crossvalidation.
Литература
1. Breiman L., Friedman J. H., Olshen R. A., Stone C. T.
Classification and Regression Trees.— Wadsworth, Belmont,
California, 1984.
2. Quinlan J. R. C 4.5 Programs for Machine Learning.—
Morgan Kaufmann, San Mateo, California, 1993.
3. Machine Learning, Neural and Statistical Classification /
Editors: D. Michie, D. J. Spiegelhalter, C. C. Taylor.—
London: Ellis Horwood, 1994.
4. Distribution based trees are more accurate. L. Breiman,
N. Shang.— ftp://ftp.stat.berkeley.edu.
53

assification and Regression Trees.— Wadsworth, Belmont,
California, 1984.
2. Quinlan J. R. C 4.5 Programs for Machine Learning.—
Morgan Kaufmann, San Mateo, California, 1993.
3. Machine Learning, Neural and Statistical Classification /
Editors: D. Michie, D. J. Spiegelhalter, C. C. Taylor.—
London: Ellis Horwood, 1994.
4. Distribution based trees are more accurate. L. Breiman,
N. Shang.— ftp://ftp.stat.berkeley.edu.
53

