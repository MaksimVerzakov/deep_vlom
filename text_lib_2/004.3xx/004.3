УДК 004.3
БАКАНОВ В.М.
ПОТО́КОВЫЕ (DATA-FLOW) ВЫЧИСЛИТЕЛИ:
УПРАВЛЕНИЕ ИНТЕНСИВНОСТЬЮ ОБРАБОТКИ ДАННЫХ
Проведено имитационное компьютерное моделирование вычислителя потоко-
вой (DATA-FLOW) архитектуры, выявлены недостатки процесса обработки дан-
ных и предложены стратегии их устранения.
A computer simulation of the calculator stream (DATA-FLOW) architecture, the iden-
tified weaknesses of the data and suggested strategies to address them.
Развитие науки и технологий требует постоянного повышения вы-
числительных мощностей цифровых ЭВМ для решения насущных задач.
Повышение
быстродействия
процессоров
традиционной
(фон-
Неймановской) архитектуры в настоящее время ограничивается известны-
ми физическими законами, разработка новых направлений в этом контек-
сте требует огромных усилий и очень дорогостояща. Практически единст-
венной реальной альтернативой повышению мощности единого процессо-
ра в настоящее время является параллельность в вычислениях [1].
Существующие параллельные системы включают до десятков тысяч
отдельных вычислительных узлов (включая многоя́дерные), параллельно
выполняющих программу. Недостатком такого подхода является необхо-
димость большого труда в разработке параллельных программ (паралле-
лизм в программах обычно является скрытым и трудновыявляемым); сис-
темы автоматизации распараллеливания недостаточно эффективны для ал-
горитмов общего назначения.
Представляет интерес рассмотреть архитектуры вычислительных
систем, осуществляющих аппаратное распараллеливание на уровне ма-
шинных команд. На рубеже 70-х г.г. Джек Деннис (Jack Dennis) изложил
фундаментальные принципы управления процессом вычислений пото́ком
данных (DATA-FLOW) в противове́с программному управлению вычисле-
ниями (CONTROL-FLOW). В фон-Неймановских вычислителях процессом
вычислений на уровне машинных инструкций управляет регистр – счетчик
команд (СК), определяющий номер выбираемой из памяти и выполняемой
команды; путем модификации содержимого СК реализуются условные и
безусловные переходы, циклы. Однако именно СК является “слабым зве-
ном” архитектуры, фактически препятствующим распараллеливанию про-
граммы на уровне инструкций (вследствие явного указания СК в данный
момент на одну-единственную команду).
Основополагающим понятием пото́ковых вычислений является
принцип гото́вности к выполнению операции по условию гото́вности всех
необходимых для выполнения этой операции операндов. Операнд “готов”,
если соответствующим операндам ячейкам памяти присвоено значение
(вычислено предыдущими операторами или определено константой).
В качестве устройства управления вычислениями выступает (вместо
СК) коммутатор, получающий информацию об изменении значений опе-
рандов на предмет выявления готовых к выполнению операций и пере-
дающий их имеющимся в системе исполнительным устройствам (ариф-
метическим, устройствам ввода/вывода и т.п.), вновь вычисленные значе-
ния определяют готовность следующих операций и т.д. Если в системе
имеется достаточное количество процессоров, все готовые в данный мо-
мент к выполнению команды будут переданы на исполнение, что и позво-
ляет говорить об истинно массовом (причём реализуемым не программ-
ным, а аппаратным – посредством функционирования архитектуры) па-
раллелизме на уровне машинных инструкций в DATA-FLOW системах.
Между памятью и исполнительными устройствами (коммутаторами, про-
цессорами) циркулируют токены (комбинации данных c дополнительны-
ми флагами, указывающими на признаки данных - готовность к выполне-
нию операторов и др.). Интересно, что в подобной системе, вообще го-
воря, порядок расположения в памяти кодов операций несущественен (он
автоматически определяется на этапе выполнения). Уровень грану-
ля́рности такой программы наименьший из представи́мых (гранула = одна
процессорная инструкция); возможны и много бо́льшие гранулы (сот-
ни/тысячи инструкций), однако выявление таких гранул вряд ли возможно
аппаратно и в этом случае нагрузка на выявле́ние таких гранул ложи́тся на
компилятор.
Хотя исторически магистральным направлением архитектуры вы-
числителей (процессоров) стала классическая архитектура фон Неймана
(von Nueman), исследования в области не-фон Неймановских вычислите-
лей никогда не прекращались. В России до своей кончи́ны в 2005 г. реали-
зацией пото́ковых (DATA-FLOW) вычислителей занимался Бурцев В.C.
[2]. Начало массового выпуска 64  100 ядерных процессоров на едином
кристалле (Tilera, nVidia, Intel) может рассматриваться в качестве начала
перехода к вычислителям архитектуры DATA-FLOW.
Упрощённо схема пото́кового вычислителя приведена на рис.1 (па-
мять инструкций - ПИ и память данных – ПД изображены отдельно в це-
лях упрощения). Вычислительное устройство имеет кольцевую структуру;
ПИ и ПД физически представляют единую сверхбыстродействующую ас-
социативную память (АП), в которой хранятся данные с признаками
(то́кены). “Готовые к выполнению” машинные инструкции с помощью
входного коммутатора передаются исполнительным устройствам. Если
все исполнительные устройства в данный момент заняты, готовые инст-
рукции накапливаются в буферной памяти; результаты выполнения инст-
рукций через выходной коммутатор загружаются опять же в АП.
Для рассмотрения работы DATA-FLOW машины удобно представить
информационный граф алгоритма в ярусно-параллельной форме (ЯПФ) –
с выделением операций, зави́симых исключительно от операндов, вычис-
ленных на верхнем (по отношению к данному) ярусе; т.е. мо́гущие выпол-
няться параллельно); рис.2. В DATA-FLOW вычислителе выявле́ние яру-
сов параллелизации выполняется автоматически на аппаратном уровне.
Рисунок 1 - Упрощенная схема пото́кового вычислителя, принятая
при компьютерном моделировании
Разработанный на кафедре компьютерный симуля́тор DATA-FLOW
машины [3] служит целям исследовательской работы и учебного процесса,
обладает некоторыми ограничениями на структуру алгоритмов, выполняет
определённую систему команд и позволяет моделировать процесс с учётом
относительного времени выполнения машинных инструкций. Ход расчёта
протоколируется, данные импортируются в EXCEL для анализа в графи-
ческой форме.
В качестве примера исследований на рис.3 приведён график функции
числа “готовых к выполнению” операций от времени работы программы
(пунктиром показана некоторым образом сгла́женная кривая) для кон-
кретного алгоритма. Для характеристики распределе́ния числа параллель-
но исполняемых команд по времени выполнения программы введено поня-
тие функции интенсивности вычислений I(t) - количества одновременно
выполняемых операций; тогда кумулятивная (нако́пленная) кривая N(t)
количества выполненных операций с начала вычислительного процесса до
t
текущего времени t определяется как N ( t )   I ( t ) dt (само собой, что
0
t0
 I ( t ) dt  N , где t0 – время выполнения программы, N – общее число
0
операций).
Рисунок 2 - Представле́ние программы в ярусно-параллельной
форме (ЯПФ)
Анализ данных рис.3 приводит к выводу о значительной неравно-
мерности интенсивности вычислений (ИВ) по мере выполнения програм-
мы. Аналогичная приведённой крайне неравномерно распределённая
функция I(t) в высшей степени нерациональна с точки зрения нагрузки на
шины данных пото́кового вычислителя. Идеалом было бы стремле́ние реа-
лизовать max[I(t)]  N / t0 (усреднённое значение интеграла) при условии
минимального увеличе́ния общего времени выполнения программы.
Логично предположить, что форма кривой интенсивности вычисле-
ний I(t) определяется внутренними свойствами алгоритма, а её экстре-
мум – размером обрабатываемых данных. На основе экспериментов вве-
дено [3] понятие кумулятивной кривой числа испо́лненных операций, яв-
ляющееся расширением (в части анализа закона накопле́ния операций по
мере выполнения алгоритма) традиционно рассматриваемой функции вы-
числительной трудоёмкости.
Рисунок 3 - Число “готовых к выполнению” операций (интенсивность
вычислений) в функции времени выполнения программы (решение
СЛАУ 5-го порядка прямым методом Гаусса)
Следующим этапом исследований является определение числа про-
цессоров, необходимых для полного распараллеливания алгоритма (т.е.
анализ величины max[I(t)]) при различных объёмах исходных данных);
пример результатов экспериментов – рис.4.
Опыты показывают, что неравномерность интенсивности вычисле-
ний резко (квадратично) возрастает с повышением разме́рности задачи; не
менее интенсивно увеличивается нагрузка на шины данных, объединяю-
щие элементы потокового вычислителя.
В случае многозадачной работы пото́ковой машины (схема рис.5)
изменение суммарной нагрузки по времени сглаживается, оставаясь, одна-
ко, неравномерной (зависит, конечно, от характера решаемых задач и стра-
тегии управления выполнением о́ных).
В общем случае j-тую функцию интенсивности вычислений можно
представить подходя́щей (в смысле метода Ритца) функцией с тремя па-
раметрами – s4, l4 и m4 (момент времени начала процесса, его продолжи-
тельности и экстремум соответственно, см. пример на рис.5 для случая
j=4); тогда (исходя из принципа суперпозиции) общая нагрузка IΣ(t) на по
то́ковый вычислитель будет равна I  (t )   Ij (t ) . Именно такая аксио-
j
матика принята для моделирования работы пото́кового вычислителя в дан-
ном исследовании, при этом коне́чный результат моделирования определя-
ется (априори неизвестным, но мо́гущим быть фиксированным на основе
как “разумных предположений”, так и конкретной эмпи́рики) законом рас-
пределения значений s4, l4 и m4 .
Рисунок 4 - Зависимость интенсивности вычислений от размеров об-
рабатываемых данных (2,3,4,5 – порядок решаемых СЛАУ) при
достаточном количестве процессоров и при ограниченном их ко-
личестве (5/12 и 5/6 – решение СЛАУ 5-того порядка на 12 и 6 про-
цессорах соответственно)
При достаточном количестве ИУ в потоковой машине все “готовые”
операторы немедленно направляются на свободные ИУ; при этом ИВ мак-
симальная и время решения задачи минимально. В реальном случае число
ИУ ограничено, часть “готовых” операций становится в очередь и выпол-
нение программы замедляется (такой же эффект имеем при искусственном
– с целью уменьшения нагрузки на шины данных – снижении ИВ). В по-
следнем случае особенно полезно иметь технологию, позволяющую управ-
лять величиной интенсивности вычислений (перераспределять функцию
ИВ по времени выполнения программы).
С целью выработки стратегии управления ИВ введено понятие “по-
лезности” (с точки зрения выполнения конечной цели = скорейшего за-
вершения программы) каждого оператора. “Полезность” оператора зависит
от количества иных операторов, для которых данный обеспечивает готов-
ность их входных операндов (рис.6, ср. с рис.2).
В дальнейшем на основе значения критерия “полезности” строится
стратегия определения приоритета операций для исполнения на ИУ. Ни-
же рассматривается несколько подходов к определению критерия “полез-
ности”.
В простейшем случае для оператора i “полезность” Pi определяется
числом операторов, выполнение которых зависит от его выполнения, тогда
Pi=j, где j – количество зависимых от i операторов.
Рисунок 5 - Интенсивность вычислений в случае многозадачной рабо-
ты пото́кового вычислителя: одновременно выполняются задания
I1(t) – I4(t), суммарная нагрузка на пото́ковый вычислитель IΣ(t)
Рисунок 6 - Схема к обоснованию величины “полезности” выполнения
операторов в потоковом вычислителе
Однако вес каждого входного операнда у оператора j неодинаков и
зависит от их общего числа nj (обычно nj=1–2, но в общем случае может
1
быть любым); тогда с учётом весов получаем Pi= 
.
nj
j
Интересен предикторский подход, учитывающий динамику выпол-
нения процесса вычислений и число уже “готовых” операндов у каждого
оператора j. Если зависящий от i оператор j имеет nj входных операндов,
из которых уже kj “готовы”, то “неготовыми” остаются nj-kj и
k j 1
Pi= 
j nj
(при kj=0 для каждого оператора j имеем минимальный вес
1/nj , а при kj=nj–1 вес максимален). Недостатком этого метода является
принципиальная невозможность определения kj к началу выполнения опе-
ратора j – значение kj приходится вычислять раньше (перед выполнением
оператора i), а за время выполнения оператора i могут выполниться дру-
гие операторы, увеличивающие kj (т.о. оценка “полезности” будет песси-
мистической – за время выполнения оператора i она повысится с ростом kj
у каждого оператора j).
Де́йственность (в смысле эффективности управления интенсивно-
стью вычислений) различных стратегий проверяется путем компьютерного
моделирования потокового (DATA-FLOW) вычислителя.
Общим недостатком предикторских стратегий является повышение
нагрузки на (и так сильнонагруженную операциями выборки “готовых”
операций) ассоциативную память (рис.1) потоковой машины, однако раз-
витие аппаратных технологий постепенно “сведёт на нет” этот недостаток.
Список литературы
1. Воеводин В.В., Воеводин Вл.В. Параллельные вычисления. –
С.Птб.: BHV-Петербург, 2002. –608 c.
2. Бурцев В.С. Новые принципы организации вычислительных про-
цессов высокого параллелизма // Материалы Международной научно-
технической конференции “Интеллектуальные и многопроцессорные сис-
темы – 2003”. Том 1. Таганрог: Изд-во ТРТУ, 2003.
3. Баканов В.М. Исследование возможностей и перспектив аппарат-
ного распараллеливания алгоритмов в пото́ковых (DATA-FLOW) вычисли-
телях. // Материалы XIII международной научно-практической конферен-
ции “Фундаментальные и прикладные проблемы приборостроения и ин-
форматики” 04–08 октября 2010 г., Сочи, Россия.
Сведения об авторах
Баканов Валерий Михайлович
д.т.н., профессор кафедры Персональные компьютеры и сети
Московский государственный университет приборостроения и информатики, г. Москва
Тел.: + 7(499)122-1328
E-mail: e881e@mail.ru

