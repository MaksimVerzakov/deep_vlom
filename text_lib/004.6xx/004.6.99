МЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫЙОД
ЙОДОПИСАНИЕ АЛГОРИТМА CART
УДК 004.6
Описание алгоритма CART
И. М. Андреев
.В статье описывается один из самых из-
вестных алгоритмов построения деревьев
решений CART. Алгоритм применяется в
активно развивающемся направлении Data
Mining — добычи знаний из баз данных.
Алгоритм предназначен для решения задач
классификации объектов и построения ре-
грессионной модели. По сравнению с ней-
росетями, обычно применяемыми для реше-
ния подобных задач, алгоритм отличает вы-
сочайшая скорость и возможность наглядной
визуализации найденного решения при срав-
нимой точности классификации. Статья бу-
дет полезна специалистам, интересующимся
направлением Data Mining, и разработчикам,
применяющим алгоритмы Data Mining в своих
продуктах.
CART (сокращение от Classification And Regression
Tree) переводится как «Дерево Классификации и Регрес-
сии» — алгоритм бинарного дерева решений, впервые
опубликованный Бриманом и др. в 1984 году [1]. Алго-
ритм предназначен для решения задач классификации
и регрессии. Существует также несколько модифициро-
ванных версий — алгоритмы IndCART и DB-CART. Ал-
горитм IndCART является частью пакета Ind и отлича-
ется от CART использованием иного способа обработ-
ки пропущенных значений, он не реализует регрессион-
ную часть алгоритма CART и имеет иные параметры
отсечения. Алгоритм DB-CART базируется на следую-
щей идее: вместо того, чтобы использовать обучающий
набор данных для определения разбиений, оцениваются
распределения входных и выходных значений, и затем
на основе полученных оценок определяются разбиения.
Утверждается, что эта идея дает значительное умень-
шение ошибки классификации по сравнению со стан-
дартными методами построения дерева. Основные от-
личия алгоритма CART от алгоритмов семейства ID3
(http://glossary.basegroup.ru/a/id3.htm) заключены
в следующих представлениях, функциях и механизмах:
бинарном представленим дерева решений;
функции оценки качества разбиения;
механизме отсечения дерева;
алгоритме обработки пропущенных значений;
построении деревьев регрессии.
Бинарное представление дерева
решений
В алгоритме CART каждый узел дерева решений име-
ет двух потомков. На каждом шаге построения дерева
правило, формируемое в узле, делит заданное множе-
48
ство примеров (обучающую выборку) на две части —
часть, в которой выполняется правило (потомок — right),
и часть, в которой правило не выполняется (потомок —
left). Для выбора оптимального правила используется
функция оценки качества разбиения.
Предлагаемое алгоритмическое решение
Каждый узел (структура или класс) должен иметь
ссылки на двух потомков: Left и Right (структуры анало-
гичного вида). Узел также должен содержать идентифи-
катор правила (подробнее о правилах см. ниже), каким-
либо образом описывать правую часть правила, содер-
жать информацию о количестве или отношении приме-
ров каждого класса обучающей выборки, «прошедшей»
через узел, и иметь признак терминального узла — ли-
ста. Таковы минимальные требования к структуре (клас-
су) узла дерева.
Функция оценки качества разбиения
Построение дерева решений относится к классу обу-
чения с учителем, то есть обучающая и тестовая выбор-
ки содержат классифицированный набор примеров. Оце-
ночная функция, используемая алгоритмом CART, ба-
зируется на интуитивной идее уменьшения «нечистоты»
(неопределенности) в узле. Имеется в виду такое разбие-
ние, при котором в узле будет как можно больше приме-
ров одного класса и как можно меньше всех других. Это
очень похоже на термин «неопределенность» — энтро-
пия. Но дело в том, что тут используется другая (не эн-
тропийная) мера неопределенности. Потому и пришлось
использовать термин «нечистота».
АВТОР
Андреев Иван
Михайлович
Инженер-программист;
RBaseGroup Labs, г. Рязань
Рассмотрим задачу с двумя классами и узлом, име-
ющим по 50 примеров одного класса. Пусть узел имеет
максимальную «нечистоту». Если будет найдено разби-
ение, которое разбивает данные, например, на две под-
группы (40:5 примеров в одной и 10:45 в другой), то ин-
туитивно «нечистота» уменьшится. Она полностью ис-
чезнет, когда будет найдено разбиение, которое создаст
подгруппы 50:0 и 0:50. В алгоритме CART идея «нечи-
стоты» формализована в индексе Gini. Если набор дан-
ных T содержит данные n классов, тогда индекс Gini
#3—4 (7—8) 2004
ЙОДМЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫ
И. М. АНДРЕЕВЙОД
Предлагаемое алгоритмическое решение
определяется как
n
p2 ,
i
Gini(T ) = 1 −
i=1
где параметр pi — вероятность класса i в T .
Если набор T разбивается на две части T1 и T2 с чис-
лом примеров в каждом N1 и N2 соответственно, то по-
казатель качества разбиения будет равен
Ginisplit (T ) =
N1
N2
Gini(T1 ) +
Gini(T2 ).
N
N
РЕЗЮМЕ
Наилучшим считается то разбиение, для которого
Ginisplit (T ) минимально.
Обозначим через N число примеров в узле-предке, че-
рез L и R — число примеров в левом и правом потомке
соответственно, li и ri — число экземпляров i-го клас-
са в левом/правом потомке. Тогда качество разбиения
оценивается по следующей формуле:
n
L R
N 1−
   N
2
li
L
1−
+ 
Ginisplit =
i=1
n
ri
R
i=1
+
2
→ min.
Чтобы уменьшить объем вычислений, формулу для
Ginisplit можно переписать иначе:
Ginisplit =
n
1
1
L 1− 2
N
L
2
li
+R 1−
i=1
1
R2
n
2
ri
.
i=1
Так как умножение на константу не играет роли при ми-
нимизации, то
Ginisplit = L −
Ginisplit = N −
1
 ̃
Gsplit =
L
Договоримся, что источник данных, необходимых для
работы алгоритма, представим в виде плоской таблицы,
каждая строка которой описывает один пример обучаю-
щей/тестовой выборки. Каждый шаг построения дерева
фактически состоит из совокупности трех трудоемких
операций.
1
L
n
2
li + R −
i=1
1
L
n
i=1
n
2
li +
i=1
2
li +
1
R
1
R
n
1
R
n
2
ri → min,
i=1
n
2
ri
→ min,
i=1
2
ri → max.
i=1
В итоге, лучшим будет то разбиение, для которо-
 ̃
го величина Gsplit максимальна. Реже в алгорит-
ме CART используются другие критерии разбиения:
Twoing, Symmetric Gini и др. (подробнее см. [1]).
Правила разбиения
Вектор предикторных переменных, подаваемый на
вход дерева, может содержать как числовые (порядко-
вые), так и категориальные переменные. В любом слу-
чае, в каждом узле разбиение идет только по одной пе-
ременной. Если переменная имеет числовой тип, то в уз-
ле формируется правило вида xi ≤ c, где c — некото-
рый порог, который чаще всего выбирается как среднее
арифметическое двух соседних упорядоченных значений
переменной обучающей выборки. Если переменная име-
ет категориальный тип, то в узле формируется прави-
ло xi ∈ V (xi ), где V (xi ) — некоторое непустое подмно-
жество множества значений переменной xi в обучающей
выборке. Следовательно, для n значений числового ат-
рибута алгоритм сравнивает n − 1 разбиений, а для ка-
тегориального — (2n−1 − 1). На каждом шаге постро-
ения дерева алгоритм последовательно сравнивает все
возможные разбиения для всех атрибутов и выбирает
наилучший атрибут и наилучшее разбиение для него.
МАТЕМАТИКА В ПРИЛОЖЕНИЯХ
Задача
Data Mining.
Результаты
Подробно рассмотрен алгоритм построения деревьев
решений CART.
1. Сортировка источника данных по столбцу. Она
необходима для вычисления порога, когда рассматривае-
мый в текущий момент времени атрибут имеет числовой
тип. На каждом шаге построения дерева число сорти-
ровок будет как минимум равно количеству атрибутов
числового типа.
2. Разделение источника данных. После того как най-
дено наилучшее разбиение, необходимо разделить источ-
ник данных в соответствии с правилом формируемого
узла и рекурсивно вызвать процедуру построения для
двух половинок источника данных.
Обе эти операции связаны (если действовать напря-
мую) с перемещением данных, занимающих значитель-
ные объемы памяти. Здесь намеренно источник данных
не называется таблицей, т. к. можно существенно сни-
зить временные затраты на построение дерева, если ис-
пользовать индексированный источник данных. Обраще-
ние к данным в таком источнике происходит не напря-
мую, а посредством логических индексов строк данных.
Сортировать и разделять такой источник можно с ми-
нимальной потерей производительности.
3. Третья операция, занимающая 60–80% времени вы-
 ̃
полнения программы, — вычисление индексов Gsplit для
всех возможных разбиений. Если у вас n числовых ат-
рибутов и m примеров в выборке, то получается таблица
индексов размера n × (m − 1), которая занимает большой
объем памяти. Этого можно избежать, если использо-
вать один столбец для текущего атрибута и одну строку
для лучших (максимальных) индексов для всех атрибу-
тов. Можно и вовсе использовать только несколько чис-
ловых значений, получив быстрый, но плохо читаемый
код. Значительно увеличить производительность можно,
если принять во внимание, что L = N −R, li = ni −ri , а li
и ri изменяются всегда и только на единицу при переходе
на следующую строку для текущего атрибута. Другими
словами, подсчет числа классов (а это основная опера-
ция) будет выполняться быстро, если знать число экзем-
пляров каждого класса в таблице и при переходе на но-
вую строку таблицы изменять на единицу только число
экземпляров одного класса — класса текущего примера.
Все возможные разбиения для категориальных ат-
рибутов удобно представлять по аналогии с двоичным
представлением числа. Если атрибут имеет n уникаль-
ных значений, имеем 2n разбиений. Первое (где все ну-
ли) и последнее (все единицы) нас не интересуют, та-
ким образом, получаем 2n − 2 разбиений, а т. к. поря-
док множеств здесь тоже неважен, получаем, что чис-
ло первых (с единицы) двоичных представлений равно
(2n − 2)/2 = 2n−1 − 1. Если {A, B, C, D} — все возмож-
49
МЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫЙОД
ные значения некоторого атрибута X, то для текущего
разбиения, которое имеет представление, например, для
{0, 0, 1, 0, 1}, получаем правило X in {C, E} для правой
ветви и not {0, 0, 1, 0, 1} = {1, 1, 0, 1, 0} = X in {A, B, D}
для левой ветви.
Часто значения атрибута категориального типа пред-
ставлены в базе как строковые значения. В таком случае
быстрее и удобнее создать кэш всех значений атрибута
и работать не со значениями, а с индексами в кэше.
Механизм отсечения дерева
Наиболее серьезное отличие алгоритма CART от дру-
гих алгоритмов построения дерева заключено в меха-
низме отсечения дерева (оригинальное название minimal
cost-complexity tree pruning). CART рассматривает отсе-
чение как получение компромисса между получением де-
рева оптимального размера и получением точной оценки
вероятности ошибочной классификации.
Основная проблема отсечения — большое количество
возможных отсеченных поддеревьев для одного дерева.
Конкретнее, если бинарное дерево имеет |T | листов, то
существует ≈ 1.5028369|T | отсеченных поддеревьев. Та-
ким образом, если дерево имеет хотя бы 1000 листов, то
число отсеченных поддеревьев становится катастрофи-
чески большим.
Базовая идея метода — не рассматривать все возмож-
ные поддеревья, а ограничиться только «лучшими пред-
ставителями» согласно приведенной ниже оценке.
Обозначим через |T | число листов (терминальных уз-
лов) дерева, а через R(T ) — ошибку классификации де-
рева, равную отношению числа неправильно классифи-
цированных примеров к числу примеров в обучающей
выборке. Определим Cα (T ) — полную стоимость (оцен-
ку/показатель затраты—сложность) дерева T как
Cα (T ) = R(T ) + α|T |,
где α — некоторый параметр, изменяющийся от 0 до +∞.
Полная стоимость дерева включает две компоненты —
ошибку классификации дерева и штраф за его слож-
ность. Если ошибка классификации дерева неизменна,
то с увеличением α полная стоимость дерева будет уве-
личиваться. Тогда, в зависимости от α, менее ветвистое
дерево, дающее большую ошибку классификации, может
стоить меньше, чем дерево более ветвистое, но дающее
меньшую ошибку.
Определим Tmax — максимальное по размеру дерево,
которое предстоит обрезать. Если мы зафиксируем зна-
чение α, то будем иметь наименьшее минимизируемое
поддерево T (α), для которого выполняются следующие
условия:
1) Cα (T (α)) = minT ≤Tmax Cα (T );
2) if Cα (T ) = Cα (T (α)) then T (α) ≤ T.
Первое условие говорит, что не существует такого подде-
рева дерева Tmax , которое имело бы меньшую стоимость,
чем T (α) при заданном значении α. Из второго усло-
вия следует, что если существует более одного поддере-
ва, имеющего данную полную стоимость, то мы выбира-
ем наименьшее дерево. Можно показать, что для любого
значения α существует такое наименьшее минимизируе-
мое поддерево. Однако эта задача нетривиальна. Из ее
решения следует, что ситуация, когда два дерева дости-
гают минимума полной стоимости и являются несрав-
нимыми (т. е. ни одно из них не является поддеревом
другого), невозможна.
Несмотря на то что α имеет бесконечное число зна-
50
ЙОДОПИСАНИЕ АЛГОРИТМА CART
чений, существует конечное число поддеревьев дерева
Tmax . Следовательно, можно построить последователь-
ность уменьшающихся поддеревьев дерева Tmax :
T1 > T2 > T3 > ... > {t1 }
(где t1 — корневой узел дерева) такую, что Tk — наимень-
шее минимизируемое поддерево для α ∈ [αk , αk+1 ). Этот
важный результат показывает, что мы можем получить
следующее дерево в последовательности, применив от-
сечение к текущему дереву. Последнее позволяет разра-
ботать эффективный алгоритм поиска наименьшего ми-
нимизируемого поддерева при различных значениях α.
Первое дерево в этой последовательности — наименьшее
поддерево дерева Tmax , имеющее такую же ошибку клас-
сификации, как и Tmax , т. е. T1 = T (α = 0). Другими
словами, если разбиение идет до тех пор, пока в каж-
дом узле не останется только один класс, то T1 = Tmax .
Но так как часто применяются методы ранней останов-
ки (prepruning), то может существовать поддерево дерева
Tmax , имеющее такую же ошибку классификации.
Предлагаемое алгоритмическое решение
Алгоритм вычисления T1 из Tmax прост: надо найти
любую пару листов с общим предком, которые могут
быть объединены, т. е. отсечены в родительский узел без
увеличения ошибки классификации,
R(t) = R(l) + R(r),
где r и l — листы узла t. Эту процедуру следует продол-
жать до тех пор, пока таких пар больше не останется. В
итоге мы получим дерево, имеющее такую же стоимость,
как и Tmax при α = 0, но менее ветвистое, чем Tmax .
Здесь встает вопрос о том, как получить следующее
дерево в последовательности и соответствующее значе-
ние α. Обозначим через Tt ветвь дерева T с корневым
узлом t. При каких же значениях α дерево T − Tt бу-
дет лучше, чем T ? Если мы сделаем отсечение в узле t,
то вклад этого узла в полную стоимость дерева T − Tt
станет равным Cα ({t}) = R(t) + α. Здесь R(t) = r(t)p(t),
r(t) — ошибка классификации узла t и p(t) — пропор-
ция случаев, которые «прошли» через узел t. Альтерна-
тивный вариант: R(t) = m/n, где m — число примеров,
классифицированных некорректно, а n — общее число
классифицируемых примеров для всего дерева.
Вклад Tt в полную стоимость дерева T составит
Cα (Tt ) = R(Tt ) + α|Tt |,
где
R(Tt ) =
R(t ).
t ∈Tt
Дерево T − Tt будет лучше, чем T , в том случае, если
Cα ({t}) = Cα (Tt ), поскольку при этой величине α они
имеют одинаковую стоимость, но T − Tt меньше. Когда
Cα ({t}) = Cα (Tt ), имеем
R(Tt ) + α|Tt | = R(t) + α.
Решая уравнение относительно α, получаем
α=
R(t) − R(Tt )
.
|Tt | − 1
Для любого узла t в T1 при увеличении α, когда
α=
R(t) − R(T1,t )
,
|T1,t | − 1
дерево, полученное отсечением в узле t, будет лучше T1 .
#3—4 (7—8) 2004
ЙОДМЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫ
И. М. АНДРЕЕВЙОД
Основная идея здесь состоит в следующем: вычислим
это значение α для каждого узла в дереве T1 и затем
выберем «слабые связи» (их может быть больше, чем
одна), т. е. узлы, для которых величина
g(t) =
R(t) − R(T1,t )
|T1,t | − 1
È
является одновременно и деревом T1 , все листы не име-
ют неправильно классифицированные примеры, поэтому
R(T1,t1 ) = 5 0/200 = 0. Здесь |T1,t1 | — количество ли-
1
стов поддерева с корнем в узле t1 , число которых равно
пяти. В итоге получаем:
g1 (t1 ) = g1 (t2 ) = 
T1 = T (α = 0)
α1 = 0
k=1
while Tk > {root node} do
begin
для всех нетерминальных узлов (!листов) в t ∈ Tk
R(t) − R(T1,t )
g(t) =
|T1,t − 1|
αk+1 = mint gk (t)
Обойти сверху вниз все узлы и обрезать те,
где gk (t) = αk+1 , чтобы получить Tk+1
k =k+1
end
Узлы необходимо обходить сверху вниз, чтобы не от-
секать узлы, которые отсекутся сами собой в результате
отсечения n-го предка.
1
,
20
g1 (t5 ) = 
Алгоритм вычисления последовательности деревьев
3
,
20
g1 (t3 ) = 
является наименьшей. Затем отсечем T1 в этих узлах,
чтобы получить T2 — следующее дерево в последователь-
ности. Продолжим этот процесс для полученного дерева
до тех пор, пока не получим корневой узел (дерево, в
котором только один узел).
1/2 − 0
 1
= ,
5−1
 8
1
.
20
Узлы t3 и t5 хранят минимальное значение g1 . Чтобы
получить новое дерево T2 , нужно обрезать T1 в обоих
этих узлах. Полученное дерево изображено на рис. 2.
Продолжим вычислять значение g для T2 :
g2 (t1 ) =
100/200 − (0/200 + 10/200 + 10/200)
2
=
,
3−1
10
60/200 − (0/200 + 10/200)
1
= .
2−1
4
Поскольку минимум хранится в узле t1 , то, обрезая в
t1 , получим корень дерева (T3 = {t1 }). На этом процесс
отсечения заканчивается.
Последовательность значений α имеет следующий
вид: α1 = 0, α2 = 1/20, α3 = 2/10. В итоге T1 яв-
ляется лучшим деревом для α ∈ [0, 1/20), T2 — для
α ∈ [1/20, 2/10) и T3 = {t1 } — для α ∈ [2/10, +∞).
g2 (t2 ) =
Выбор финального дерева
Рис. 1. Дерево Tmax .
В качестве примера вычислим последовательность
поддеревьев и соответствующих значений α для дерева,
изображенного на рис. 1. Очевидно, что T1 = Tmax , т. к.
все листы содержат примеры одного класса, и отсечение
любого узла (t3 или t5 ) приведет к возрастанию ошибки
классификации. Затем вычислим g1 (t) для всех узлов t
в T:
R(t1 ) − R(T1,t1 )
g1 (t1 ) =
,
|T1,t1 | − 1
где R(T1 ) — ошибка классификации. Если t1 превратить
в лист, то следует сопоставить ему какой-либо класс.
Так как число примеров обоих классов одинаково (рав-
но 100), то класс выбираем наугад, в любом случае он
неправильно классифицирует 100 примеров. Всего де-
рево обучалось на 200-х примерах (100 + 100 = 200
для корневого узла), причем R(T1 ) = m/n, m = 100,
n = 200, R(T1 ) = 1/2. В последней формуле R(T1,t1 ) —
сумма ошибок всех листов поддерева, которая рассчи-
тывается как сумма отношений количества неправиль-
но классифицированных примеров в листе к общему
числу примеров для дерева по всем листам. В приме-
ре все делим на 200. Так как поддерево с корнем в t1
МАТЕМАТИКА В ПРИЛОЖЕНИЯХ
Выбор финального дерева заключается в выборе луч-
шего дерева из последовательности деревьев. В дальней-
шем мы будем использовать именно такое дерево. Отме-
тим, что при отсечении дерева не использовались ника-
кие другие данные, кроме тех, на которых строилось пер-
воначальное дерево. Требовались даже не сами данные,
а количество примеров каждого класса, которое «про-
шло» через узел.
Наиболее очевидным и, возможно, наиболее эффек-
тивным является выбор финального дерева посредством
тестирования на тестовой выборке. Естественно, каче-
ство тестирования во многом зависит от объема тестовой
выборки и «равномерности» данных, которые попали в
обучающую и тестовую выборки.
Часто можно наблюдать, что последовательность де-
ревьев дает сопоставимые ошибки (такой случай изоб-
ражен на рис. 3). Представленная длинная плоская по-
следовательность очень чувствительна к данным, кото-
рые будут выбраны в качестве тестовой выборки. Что-
бы уменьшить эту нестабильность, CART использует
Рис. 2. Дерево T2 .
51
МЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫЙОД
ЙОДОПИСАНИЕ АЛГОРИТМА CART
Альтернативный путь заключается в том, чтобы на
последнем шаге опять использовать (1 − SE)-правило
для выбора финального дерева из последовательности.
Алгоритм обработки пропущенных
значений
Рис. 3. Зависимость ошибки от размера дерева.
(1 − SE)-правило: выберается минимальное по размеру
дерево с Rts в пределах интервала [min Rts , min Rts +SE].
Здесь Rts — ошибка классификации дерева; SE — стан-
дартная ошибка, являющаяся оценкой реальной ошибки:
SE(Rts ) =
Ö
Rts (1 − Rts )
,
ntest
где ntest — число примеров в тестовой выборке. Ситуация
проиллюстрирована на рис. 3.
Перекрестная проверка
Перекрестная проверка (V-fold cross-validation) — ори-
гинальная и наиболее сложная часть алгоритма CART.
Этот способ выбора финального дерева используется, ко-
гда набор данных для обучения мал или каждая запись
в нем по-своему «уникальна», так что мы не можем вы-
делить выборку для обучения и выборку для тестирова-
ния.
Имея такой случай, следует построить дерево на всех
данных и вычислить α1 , α2 ,... αk и T1 > T2 > ... > TN .
Обозначим через Tk наименьшее минимизируемое под-
дерево для α ∈ [αk , αk+1 ).
Теперь возникает задача выбора дерева из последова-
тельности при условии, что все имеющиеся данные уже
использованы. Выход тут заключается в том, что вычис-
ление ошибки дерева Tk из последовательности выпол-
няется косвенным путем.
√
√
Шаг 1. Установим β1 = 0, β2 = α2 α3 , β3 = α3 α4 , ...
√
βN −1 = αN −1 αN , βN = ∞. Считается, что βk будет ти-
пичным значением для [αk , αk+1 ) и, следовательно, как
значение соответствует Tk .
Шаг 2. Разделим весь набор данных на V групп G1 ,
G2 , ... GV одинакового размера (Бриман [4] рекомендует
брать V = 10). Затем для каждой группы Gi следует
выполнить следующее.
1. Вычислить последовательность деревьев с помо-
щью описанного выше механизма отсечения на всех дан-
ных, исключая Gi , и определить T (i) (β1 ), T (i) (β2 ), ...
T (i) (βN ) для этой последовательности.
2. Вычислить ошибку дерева T (i) (βk ) на Gi . Здесь
(i)
T (βk ) означает наименьшее минимизированное подде-
рево из последовательности, построенное на всех дан-
ных, исключая Gi для α = βk .
Шаг 3. Для каждого βk суммировать ошибку T (i) (βk )
по всем Gi (i = 1, V ). Пусть βh будет с наименьшей общей
ошибкой. Так как βh соответствует дереву Th , выбира-
ем Th из последовательности, построенной на всех дан-
ных в качестве финального дерева. Показатель ошибки,
вычисленный с помощью перекрестной проверки, можно
использовать как оценку ошибки дерева.
52
Большинство алгоритмов Data Mining предполагает
отсутствие пропущенных значений. На практике это
предположение часто является неверным. Вот только
некоторые из причин, которые могут привести к пропу-
щенным данным:
1) респондент не желает отвечать на некоторые из по-
ставленных вопросов;
2) допущены ошибки при вводе данных;
3) выполнено объединение не совсем эквивалентных
наборов данных.
Часто в таких случаях исключают данные, которые со-
держат один или несколько пустых атрибутов. Однако
такой подход имеет ряд недостатков.
1. Смещение данных. Если выброшенные данные ле-
жат несколько «в стороне» от оставленных, анализ
может дать смещенные результаты.
2. Потеря мощности. Если количество исключенных
данных большое, точность прогноза сильно умень-
шается.
Если дерево используется на неполных данных, необхо-
димо решить следующие вопросы:
1) как определить качество разбиения?
2) в какую ветвь необходимо послать наблюдение, ес-
ли пропущена переменная, на которую приходится
наилучшее разбиение (построение дерева и трени-
ровка)?
Следует подчеркнуть, что наблюдение с пропущенной
меткой класса бесполезно для построения дерева и будет
выброшено.
Чтобы определить качество разбиения, CART про-
сто игнорирует пропущенные значения. Это «решает»
первую проблему. Но еще следует решить, по како-
му пути посылать наблюдение с пропущенной перемен-
ной, содержащей наилучшее разбиение. С этой целью
CART вычисляет так называемое «суррогатное» разби-
ение. Оно создает наиболее близкие к лучшему подмно-
жества примеров в текущем узле. Чтобы определить зна-
чение альтернативного разбиения как суррогатного, мы
создаем кросс-таблицу:
π(l∗ , l )
π(r∗ , l )
π(l∗ , r )
π(r∗ , r )
В этой таблице величина π(l∗ , l ) обозначает пропорцию
случаев, которые будут посланы в левую ветвь при луч-
шем s∗ и альтернативном s разбиении, и аналогично для
π(r∗ , r ). Так, π(l∗ , l ) + π(r∗ , r ) обозначает пропорцию
случаев, которые посланы в одну и ту же ветвь для обо-
их разбиений. Это мера сходства разбиений, характери-
зующая, насколько хорошо предсказывается путь, по ко-
торому послан случай с наилучшим разбиением по срав-
нению с альтернативным. Если π(l∗ , l ) + π(r∗ , r ) < 0.5,
можно получить лучший суррогат, поменяв левую и пра-
вую ветви для альтернативного разбиения. Кроме того,
необходимо заметить, что пропорции в таблице вычис-
лены для случая, когда обе переменные (суррогатная и
альтернативная) наблюдаемы.
Альтернативные разбиения, для которых выполняе-
ся π(l∗ , l ) + π(r∗ , r ) > max(π(r∗ ), π(l∗ )), отсортированы
в нисходящем порядке в плане сходства. Теперь, если
пропущена переменная лучшего разбиения, используем
первую из суррогатных в списке, если пропущена и она,
#3—4 (7—8) 2004
ЙОДМЕТОДЫ. АЛГОРИТМЫ. ПРОГРАММЫ
И. М. АНДРЕЕВЙОД
то следующую, и т. д. Если пропущены все суррогатные
переменные, используем max(π(r∗ ), π(l∗ )).
На рис. 4 изображен пример лучшего и альтернатив-
ного разбиения. Каково значение альтернативного раз-
биения по супружескому статусу как суррогатного? При-
меры 6 и 10 послали оба разбиения влево. Следователь-
но, π(l∗ , l ) = 2/7. Оба разбиения послали примеры 1 и
4 направо — π(r∗ , r ) = 2/7. Его значение как суррогат-
ного есть π(l∗ , l ) + π(r∗ , r ) = 2/7 + 2/7 = 4/7. Так как
max(π(r∗ ), π(l∗ )) = 4/7, разбиение по супружескому ста-
тусу не является хорошим суррогатным разбиением.
Регрессия
Построение дерева регрессии во многом схоже с по-
строением дерева классификации. Сначала строится де-
рево максимального размера, которое затем обрезается
до оптимального размера.
Основное достоинство деревьев по сравнению с други-
ми методами регрессии — возможность работать с мно-
гомерными задачами (см. рис. 5) и задачами, в которых
присутствует зависимость выходной переменной от пе-
ременной или переменных категориального типа.
Основная идея — разбиение всего пространства на
прямоугольники (необязательного одинакового разме-
ра), в которых выходная переменная считается постоян-
ной. При этом существует сильная зависимость между
объемом обучающей выборки и ошибкой ответа дерева.
Процесс построения дерева происходит последова-
тельно. На первом шаге получаем регрессионную оцен-
ку как константу по всему пространству примеров. Кон-
станту вычисляем как среднее арифметическое выход-
ной переменной в обучающей выборке. Если обозначить
все значения выходной переменной как Y1 , Y2 , ... Yn , то
регрессионная оценка получается в виде
ˆ
f (x) =
1
n
n
Yi
IR (x),
i=1
где R — пространство обучающих примеров; n — чис-
ло примеров; IR (x) — индикаторная функция простран-
ства (фактически, набор правил, описывающих попада-
ние переменной x в пространство); пространство R рас-
сматривается как прямоугольник. На втором шаге про-
странство делится на две части. Выбирается некоторая
переменная xi и, если она принадлежит числовому типу,
определяются
R1 = {x ∈ R : x ≤ a},
R2 = {x ∈ R : x > a}.
Если xi принадлежит категориальному типу и имеет воз-
можные значения A1 , A2 , .. Aq , то выбирается некоторое
подмножество I ⊂ {A1 , ...An } и определяется
R1 = {x ∈ R : x ∈ I},
R2 = {x ∈ R : x ∈ {A1 , A2 , ...Aq }\ I}.
Рис. 4. Лучшее и альтернативное разбиения.
МАТЕМАТИКА В ПРИЛОЖЕНИЯХ
Рис. 5. Пример двумерной задачи.
Регрессионная оценка принимает вид:
ˆ
f (x) =
1
|I1 |
Yi
IR1 (x) +
I1
1
|I2 |
Yi
IR2 (x),
I2
где I1 = {i, xi ∈ R1 } и |I1 | — число элементов в I1 .
Оценкой лучшего разбиения служит сумма квадратов
разностей
n
E=
2
ˆ
Yi − f (xi ) ,
i=1
которую нужно минимизировать. Процесс разбиения
продолжается до тех пор, пока в каждом подпростран-
стве не останется малое число примеров или пока сумма
квадратов разностей не станет меньше некоторого задан-
ного порога.
Отсечение и выбор финального дерева производятся
аналогично тем же процедурам для дерева классифика-
ции. Единственное отличие — определение ошибки отве-
та дерева
n
2
1
ˆ
ˆ
Yi − f (xi ) ,
R(f ) =
n i=1
или, иначе говоря, среднеквадратичной ошибки ответа.
Стоимость дерева равна
ˆ
ˆ
ˆ
Cα (f ) = R(f ) + α|f |.
Остальные операции происходят аналогично построению
дерева классификации.
Заключение
Алгоритм CART успешно сочетает в себе качество по-
строенных моделей и, при удачной реализации, высокую
скорость их построения. Также он включает в себя уни-
кальные методики обработки пропущенных значений и
построения оптимального дерева на основе совокупно-
сти алгоритмов cost-complexity pruning и V-fold cross-
validation.
Литература
1. Breiman L., Friedman J. H., Olshen R. A., Stone C. T.
Classification and Regression Trees.— Wadsworth, Belmont,
California, 1984.
2. Quinlan J. R. C 4.5 Programs for Machine Learning.—
Morgan Kaufmann, San Mateo, California, 1993.
3. Machine Learning, Neural and Statistical Classification /
Editors: D. Michie, D. J. Spiegelhalter, C. C. Taylor.—
London: Ellis Horwood, 1994.
4. Distribution based trees are more accurate. L. Breiman,
N. Shang.— ftp://ftp.stat.berkeley.edu.
53

